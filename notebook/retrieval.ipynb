{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7af3da",
   "metadata": {},
   "source": [
    "# contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ba1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_support.utils import load_yaml\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "CONFIG=load_yaml(\"config/config.yaml\")\n",
    "\n",
    "@dataclass\n",
    "class RetrievalConstants:\n",
    "    GEMINI_PRO_NAME=CONFIG.MODELS.REASONER.GEMINI_PRO\n",
    "    DEEPSEEK_R1_NAME=CONFIG.MODELS.REASONER.DEEPSEEK_R1\n",
    "    PROMPT_TEMPLATES = {\n",
    "        \"product_bot\": \"\"\"\n",
    "        You are an expert EcommerceBot specialized in product recommendations and handling customer queries.\n",
    "        Analyze the provided product titles, ratings, and reviews to provide accurate, helpful responses.\n",
    "        Stay relevant to the context, and keep your answers concise and informative.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        YOUR ANSWER:\n",
    "        \"\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e40c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GEMINI_PRO_NAME: {RetrievalConstants.GEMINI_PRO_NAME}\")\n",
    "print(f\"DEEPSEEK_R1_NAME: {RetrievalConstants.DEEPSEEK_R1_NAME}\")\n",
    "print(\"PROMPT_TEMPLATES:\", RetrievalConstants.PROMPT_TEMPLATES[\"product_bot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83df5a",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34befabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class Retrieval:\n",
    "    GEMINI_PRO_NAME:str\n",
    "    DEEPSEEK_R1_NAME:str\n",
    "    PROMPT_TEMPLATES:dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37722463",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab467f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class RetrievalConfig:\n",
    "    GEMINI_PRO_NAME=RetrievalConstants.GEMINI_PRO_NAME\n",
    "    DEEPSEEK_R1_NAME=RetrievalConstants.DEEPSEEK_R1_NAME\n",
    "    PROMPT_TEMPLATES=RetrievalConstants.PROMPT_TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GEMINI_PRO_NAME: {RetrievalConfig.GEMINI_PRO_NAME}\")\n",
    "print(f\"DEEPSEEK_R1_NAME: {RetrievalConfig.DEEPSEEK_R1_NAME}\")\n",
    "print(\"PROMPT_TEMPLATES:\", RetrievalConfig.PROMPT_TEMPLATES[\"product_bot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352bc92",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f47254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_support.components.data_ingestion import DataIngestionComponents\n",
    "from customer_support.configuration import DataIngestionConfig\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from customer_support.exception import CustomException \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from customer_support.logger import logging \n",
    "from langchain_groq import ChatGroq\n",
    "from dataclasses import dataclass \n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class RetrievalComponents:\n",
    "    __retrieval_config:Retrieval\n",
    "\n",
    "    def get_retriever(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"In get_retriever\")\n",
    "            self.data_ingestion_components=DataIngestionComponents(DataIngestionConfig)\n",
    "            self.retriever=self.data_ingestion_components.get_vector_store().as_retriever()\n",
    "\n",
    "            logging.info(\"Out get_retriever\")\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def get_LLM(self):\n",
    "        \"\"\"loads large language model either gemini_pro or deepseek_r1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In get_LLM\")\n",
    "            try:\n",
    "                self.model_name=self.__retrieval_config.GEMINI_PRO_NAME\n",
    "                self.llm=ChatGoogleGenerativeAI(model=self.model_name)\n",
    "                logging.info(f\"{self.model_name} loaded\")\n",
    "            except:\n",
    "                self.model_name=self.__retrieval_config.DEEPSEEK_R1_NAME\n",
    "                self.llm=ChatGroq(model=self.model_name)\n",
    "                logging.info(f\"{self.model_name} loaded\")\n",
    "\n",
    "            logging.info(\"Out get_LLM\")\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def get_chain(self):\n",
    "        \"\"\"create the chain\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In get_chain\")\n",
    "            self.prompt=ChatPromptTemplate.from_template(self.__retrieval_config.PROMPT_TEMPLATES[\"product_bot\"])\n",
    "            self.chain=(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "                | self.prompt\n",
    "                | self.llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            logging.info(\"Out get_chain\")\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def invoke(self, query:str) -> str:\n",
    "        \"\"\"invokes the chain\n",
    "\n",
    "        Args:\n",
    "            query (str): user query\n",
    "\n",
    "        Returns:\n",
    "            str: output as string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In invoke\")\n",
    "            result=self.chain.invoke(query)\n",
    "            logging.info(\"Out invoke\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def main(self) -> None:\n",
    "        self.get_retriever()\n",
    "        self.get_LLM()\n",
    "        self.get_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56c84e",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cde797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalPipeline:\n",
    "    def __init__(self) -> None:\n",
    "        self.retrieval=RetrievalComponents(RetrievalConfig)\n",
    "        self.retrieval.main()\n",
    "        \n",
    "    def run(self, query:str) -> str:\n",
    "        \"\"\"invokes the pipeline with given query\n",
    "\n",
    "        Args:\n",
    "            query (str): query to invoke pipeline\n",
    "\n",
    "        Returns:\n",
    "            str: string output of chain\n",
    "        \"\"\"\n",
    "        return self.retrieval.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea8d96",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d733aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    retrieval_pipeline = RetrievalPipeline()\n",
    "    query = \"Can you tell me the low budget headphone?\"\n",
    "    result=retrieval_pipeline.run(query)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730a3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
